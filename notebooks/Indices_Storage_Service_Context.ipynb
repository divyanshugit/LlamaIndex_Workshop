{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgpETdM4kUQE"
      },
      "source": [
        "# Topics Covered\n",
        "\n",
        "1.   VectorStoreIndex   \n",
        "2.   Storage Context\n",
        "3.   Service Context\n",
        "4.   List Index\n",
        "5.   Keyword Table Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCSHH5APclm6",
        "outputId": "3147b9d0-68df-45c5-bfbc-0a9f3b939383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index==0.7.0\n",
            "  Downloading llama_index-0.7.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.2/566.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from llama-index==0.7.0)\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from llama-index==0.7.0)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langchain>=0.0.218 (from llama-index==0.7.0)\n",
            "  Downloading langchain-0.0.273-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=2.0.15 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.7.0) (2.0.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index==0.7.0) (1.23.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.7.0) (8.2.3)\n",
            "Collecting openai>=0.26.4 (from llama-index==0.7.0)\n",
            "  Downloading openai-0.27.9-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index==0.7.0) (1.5.3)\n",
            "Collecting urllib3<2 (from llama-index==0.7.0)\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.7.0) (2023.6.0)\n",
            "Collecting typing-inspect==0.8.0 (from llama-index==0.7.0)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting typing-extensions==4.5.0 (from llama-index==0.7.0)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.7.0) (4.11.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from llama-index==0.7.0) (1.5.7)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect==0.8.0->llama-index==0.7.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218->llama-index==0.7.0) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218->llama-index==0.7.0) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218->llama-index==0.7.0) (4.0.3)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading langsmith-0.0.26-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218->llama-index==0.7.0) (2.8.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218->llama-index==0.7.0) (2.2.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.218->llama-index==0.7.0) (2.31.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index==0.7.0)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama-index==0.7.0) (4.66.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0.15->llama-index==0.7.0) (2.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->llama-index==0.7.0) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.7.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.7.0) (2023.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama-index==0.7.0) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index==0.7.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index==0.7.0) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index==0.7.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index==0.7.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index==0.7.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.218->llama-index==0.7.0) (1.3.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index==0.7.0) (23.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain>=0.0.218->llama-index==0.7.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain>=0.0.218->llama-index==0.7.0) (2.6.1)\n",
            "INFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pydantic<3,>=1 (from langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.6.3 (from pydantic<3,>=1->langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic_core-2.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<3,>=1 (from langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic-2.2.0-py3-none-any.whl (373 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.2/373.2 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.6.0 (from pydantic<3,>=1->langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic_core-2.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<3,>=1 (from langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.9/370.9 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.4.0 (from pydantic<3,>=1->langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic_core-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<3,>=1 (from langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic-2.1.0-py3-none-any.whl (370 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.8/370.8 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading pydantic-2.0.3-py3-none-any.whl (364 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.0/364.0 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.3.0 (from pydantic<3,>=1->langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic_core-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<3,>=1 (from langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic-2.0.2-py3-none-any.whl (359 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.1/359.1 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.1.2 (from pydantic<3,>=1->langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic_core-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<3,>=1 (from langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic-2.0.1-py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.4/358.4 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.0.2 (from pydantic<3,>=1->langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic_core-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pydantic<3,>=1 (from langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic-2.0-py3-none-any.whl (355 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.6/355.6 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.0.1 (from pydantic<3,>=1->langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic_core-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<3,>=1 (from langchain>=0.0.218->llama-index==0.7.0)\n",
            "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index==0.7.0) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.218->llama-index==0.7.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.218->llama-index==0.7.0) (2023.7.22)\n",
            "Installing collected packages: urllib3, typing-extensions, mypy-extensions, marshmallow, typing-inspect, pydantic, tiktoken, openai, langsmith, dataclasses-json, langchain, llama-index\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.2.1\n",
            "    Uninstalling pydantic-2.2.1:\n",
            "      Successfully uninstalled pydantic-2.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic-core 2.6.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.5.14 langchain-0.0.273 langsmith-0.0.26 llama-index-0.7.0 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.27.9 pydantic-1.10.12 tiktoken-0.4.0 typing-extensions-4.5.0 typing-inspect-0.8.0 urllib3-1.26.16\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index==0.7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIl8EJUumPS2"
      },
      "source": [
        "# VectorStoreIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "XJvcV-xHqADU",
        "outputId": "6c3990d0-ce30-404c-ce5a-6a9bc2ea7f0e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"font-size:20px\">\n",
              "Growing up, the author wrote short stories, programmed on an IBM 1401, wrote simple games and a word processor on a TRS-80, studied philosophy in college, wrote essays online, had dinners for a group of friends every Thursday night, bought a building in Cambridge, worked on spam filters, painted, and started a company called Viaweb.</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "from IPython.display import display, HTML\n",
        "import openai\n",
        "openai.api_key = 'YOUR_OPENAI_API_KEY'\n",
        "\n",
        "# Load documents and build index\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "query_engine = index.as_query_engine(similarity_top_k=3)\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "\n",
        "# print the synthesized response.\n",
        "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi845ettPl7Z"
      },
      "source": [
        "# Storage Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOGBYPEo1WGf"
      },
      "outputs": [],
      "source": [
        "from llama_index import StorageContext, load_index_from_storage\n",
        "from llama_index.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.storage.index_store import SimpleIndexStore\n",
        "from llama_index.vector_stores import SimpleVectorStore\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "\n",
        "# create parser and parse document into nodes\n",
        "parser = SimpleNodeParser()\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# create storage context using default stores\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    docstore=SimpleDocumentStore(),\n",
        "    vector_store=SimpleVectorStore(),\n",
        "    index_store=SimpleIndexStore(),\n",
        ")\n",
        "\n",
        "# # build index\n",
        "index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "\n",
        "# save index\n",
        "index.storage_context.persist(persist_dir=\"storage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "id": "l4MLZ6dO1T1_",
        "outputId": "4632f13c-1f3a-483f-97b3-15b1f5ea127e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"font-size:20px\">\n",
              "Growing up, the author wrote short stories, programmed on an IBM 1401, wrote simple games and a word processor on a TRS-80, studied philosophy in college, wrote essays online, had dinners for a group of friends every Thursday night, bought a building in Cambridge, worked on spam filters, painted, and started a company called Viaweb.</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# to load index later, make sure you setup the storage context\n",
        "# this will loaded the persisted stores from persist_dir\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
        "\n",
        "# then load the index object\n",
        "# if loading multiple indexes from a persist dir\n",
        "loaded_index = load_index_from_storage(storage_context)\n",
        "\n",
        "# setup query engine\n",
        "query_engine = loaded_index.as_query_engine(similarity_top_k=3)\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "\n",
        "# print the synthesized response.\n",
        "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7FFeGaPSPYg"
      },
      "source": [
        "# Service Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "id": "qcy7P-UXSTT2",
        "outputId": "d588d55f-46b8-4183-9b12-53b300a25214"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"font-size:20px\">The author wrote simple games, a program to predict how high their model rockets would fly, and a word processor that their father used to write a book. They also learned how to cook for groups by preparing dinners for a group of friends every Thursday night.</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "\n",
        "llm = OpenAI(model='gpt-4', temperature=0, max_tokens=256)\n",
        "embed_model = OpenAIEmbedding()\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=40, chunk_overlap=0)\n",
        "\n",
        "\n",
        "prompt_helper = PromptHelper(\n",
        "  context_window=4096,\n",
        "  num_output=512,\n",
        "  chunk_overlap_ratio=0.1,\n",
        ")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "  llm=llm,\n",
        "  embed_model=embed_model,\n",
        "  node_parser=node_parser,\n",
        "  prompt_helper=prompt_helper\n",
        ")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, service_context = service_context)\n",
        "\n",
        "query_engine = index.as_query_engine(similarity_top_k=3)\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "\n",
        "# print the synthesized response.\n",
        "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdQIkJ3Jo_a1"
      },
      "source": [
        "# List Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "y7UdgR7L8cRU",
        "outputId": "f3b61152-1486-4be7-da94-aebf3626d07c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"font-size:20px\">\n",
              "The document is a narrative about the author's experiences with coding, writing essays, and starting a business. It follows the story of the author's journey from working on Bel, a Lisp-based programming language, to writing essays about how to choose what to work on, to his experiences with computers, art, venture capital, and Y Combinator. The essay concludes with a discussion of the concept of invented vs discovered, and how it relates to the author's experience.</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index import ListIndex\n",
        "\n",
        "# build index\n",
        "index = ListIndex.from_documents(documents)\n",
        "\n",
        "query_engine = index.as_query_engine(response_mode='tree_summarize')\n",
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "\n",
        "# print the synthesized response.\n",
        "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "7FYraJzY8d5g",
        "outputId": "3473f566-56a9-44a3-f508-f81316bb1cf9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"font-size:20px\">\n",
              "The document is a narrative about the author's experiences with coding, writing essays, and starting a business. It follows the story of the author's journey from writing and programming as a young person to pursuing a career in art, including his realization that AI as it was practiced at the time was a hoax, his decision to focus on Lisp hacking, and his eventual acceptance to RISD and the Accademia di Belli Arti in Florence. He reflects on the difficulty of making a living as an artist and the appeal of creating something that would last. The document then follows the story of the author's founding of Viaweb, a software company that allowed users to build online stores through a web browser, and his eventual decision to leave Yahoo to pursue painting. Finally, the author reflects on his experience of choosing what to work on in the past and thanks several people for reading drafts of the essay.</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query_engine = index.as_query_engine(similarity_top_k=2, response_mode='tree_summarize')\n",
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "\n",
        "# print the synthesized response.\n",
        "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmpjCyWKp2ix"
      },
      "source": [
        "# Keyword Table Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "PmNriNJ6_hTc",
        "outputId": "accae0fd-7f5e-4b57-fc79-97490904dfa4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<p style=\"font-size:20px\">\n",
              "No, the author did not complete his graduation from Stanford. This is not mentioned in the context information.</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index import KeywordTableIndex\n",
        "\n",
        "# build index\n",
        "index = KeywordTableIndex.from_documents(documents)\n",
        "\n",
        "# query\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"did the author complete his graduation from Stanford?\")\n",
        "\n",
        "# print the synthesized response.\n",
        "display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5K7X_SU_7_5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
